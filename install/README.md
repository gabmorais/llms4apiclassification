# Installation

Running LLMs locally can be quite demanding on your RAM and GPU. Therefore, all experiments involving local LLMs ran on
an HPC server equipped with 8 NVIDIA A100 80GB GPUs and approximately 1TB of RAM. If you do not have access to an HPC,
however, experiments involving LLMs accessed via an API can still be run on a wide variety of computers.

- [`local`](local): Code for local LLMs.
- [`remote`](remote): Code for remote LLMs (accessed via an API).
